\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}

\usepackage{multirow} % 需要添加这个包来支持表格合并
\usepackage{tabularx} % 需要添加这个包来支持自适应宽度表格
\usepackage{algorithm} % 需要添加这个包来支持 algorithm 环境
\usepackage{algorithmic}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{Heterogeneous GPU Scheduling}
% \author{IEEE Publication Technology Department


\maketitle

\section{Formal Problem Description: Simplified Heterogeneous GPU Scheduling}

\subsection{Sets and Indices}
\begin{itemize}
    \item $\mathcal{T} = \{1, 2, \dots, n\}$: Set of tasks .
    \item $\mathcal{G} = \{1, 2, \dots, m\}$: Set of available GPUs in the cluster.
    \item $i, k \in \mathcal{T}$: Indices for tasks.
    \item $j \in \mathcal{G}$: Index for GPUs.
\end{itemize}

\subsection{Parameters}
\textbf{Task Model (from Dataset):}
\begin{itemize}
    \item $L_i$: Workload of task $i$ (Floating point operations or relative units).
    \item $m_i$: Memory demand of task $i$ (GB).
    \item $d_i$: Deadline of task $i$ (Time unit).
    \item $w_i$: Weight/Priority of task $i$.
    \item $a_i$: Arrival time of task $i$.
\end{itemize}

\textbf{Resource Model (GPU Cluster):}
\begin{itemize}
    \item $v_j$: Computing speed of GPU $j$ (Workload per time unit).
    \item $M_j$: Memory capacity of GPU $j$ (GB).
\end{itemize}

\textbf{Derived Parameter:}
\begin{itemize}
    \item $p_{ij} = \frac{L_i}{v_j}$: Execution time of task $i$ if assigned to GPU $j$.
\end{itemize}

\subsection{Decision Variables}
\begin{itemize}
    \item $x_{ij} \in \{0, 1\}$: Binary variable. equals 1 if task $i$ is assigned to GPU $j$, 0 otherwise.
    \item $s_i \ge 0$: Start time of task $i$.
    \item $c_i \ge 0$: Completion time of task $i$.
    \item $y_{ik} \in \{0, 1\}$: Binary sequence variable (for tasks on the same GPU). Equals 1 if task $i$ precedes task $k$, 0 otherwise.
\end{itemize}

\subsection{Mathematical Formulation}

\paragraph{\textbf{Objective Function}}
Minimize the Total Weighted Tardiness. This balances the priority ($w_i$) and the urgency (meeting $d_i$).
\begin{equation}
    \text{Minimize } Z = \sum_{i \in \mathcal{T}} w_i \cdot \max(0, c_i - d_i)
\end{equation}

\paragraph{Constraints}

1. Assignment Constraint:
Each task must be assigned to exactly one GPU.
\begin{equation}
    \sum_{j \in \mathcal{G}} x_{ij} = 1, \quad \forall i \in \mathcal{T}
\end{equation}

2. Memory Constraint:
A task can only be assigned to a GPU if the GPU's memory capacity is sufficient.
\begin{equation}
    x_{ij} \cdot m_i \le M_j, \quad \forall i \in \mathcal{T}, \forall j \in \mathcal{G}
\end{equation}

3. Timing Constraints:
The completion time is the start time plus the execution time on the assigned GPU.
\begin{equation}
    c_i = s_i + \sum_{j \in \mathcal{G}} x_{ij} p_{ij}, \quad \forall i \in \mathcal{T}
\end{equation}
A task cannot start before its arrival time.
\begin{equation}
    s_i \ge a_i, \quad \forall i \in \mathcal{T}
\end{equation}

4. Non-overlapping Constraint (Disjunctive):
If two tasks $i$ and $k$ are assigned to the same GPU, they cannot overlap in time. ($H$ is a sufficiently large positive number).
\begin{equation}
\begin{aligned}
    s_i + \sum_{j \in \mathcal{G}} x_{ij} p_{ij} &\le s_k + H(3 - x_{ij} - x_{kj} - y_{ik}) \\
    s_k + \sum_{j \in \mathcal{G}} x_{kj} p_{kj} &\le s_i + H(3 - x_{ij} - x_{kj} - (1-y_{ik}))
\end{aligned}
\end{equation}
This ensures that if both are on GPU $j$, either $i$ finishes before $k$ starts or $k$ finishes before $i$ starts.

\subsection{Performance Metrics}

Based on the decision variables and parameters defined above, the following metrics are used to evaluate the scheduling performance:

\subsubsection{Total Weighted Completion Time (TWCT)}
This metric represents the overall value-weighted responsiveness of the system. Lower is better.
\begin{equation}
    \text{TWCT} = \sum_{i \in \mathcal{T}} w_i \cdot (c_i - a_i)
\end{equation}
% \textit{Note: If the focus is on the duration from arrival, Weighted Turnaround Time ($\sum w_i (C_i - a_i)$) can also be used.}

\subsubsection{Average Completion Time (ACT)}
The average time at which tasks finish execution. Lower is better.
\begin{equation}
    \text{ACT} = \frac{1}{n} \sum_{i \in \mathcal{T}} (c_i - a_i)
\end{equation}
% \textit{Alternatively, Average Turnaround Time (ATT) describes the average time a task spends in the system:}
% \begin{equation}
%     \text{ATT} = \frac{1}{n} \sum_{i \in \mathcal{T}} (C_i - a_i)
% \end{equation}

\subsubsection{Deadline Miss Count (DMC)}
The total number of tasks that failed to complete before their deadline. Lower is better.
Let $\mathbb{I}(\cdot)$ be an indicator function that equals 1 if the condition is true, and 0 otherwise.
\begin{equation}
    \text{DMC} = \sum_{i \in \mathcal{T}} \mathbb{I}(C_i > d_i)
\end{equation}

\subsubsection{Deadline Miss Rate (DMR)}
The proportion of tasks that failed to complete before their deadline. Lower is better.
\begin{equation}
    \text{DMR} = \frac{1}{n} \sum_{i \in \mathcal{T}} \mathbb{I}(C_i > d_i)
\end{equation}

\subsubsection{Average GPU Utilization ($\eta$)}
The ratio of the total effective processing time to the total active time of the cluster. Higher is better.
First, we define the \textbf{Makespan} ($C_{\max}$), which is the completion time of the last task in the system:
\begin{equation}
    C_{\max} = \max_{i \in \mathcal{T}} (C_i)
\end{equation}
The utilization is calculated as:
\begin{equation}
    \eta = \frac{\sum_{i \in \mathcal{T}} \sum_{j \in \mathcal{G}} x_{ij} \cdot p_{ij}}{m \cdot (C_{\max} - \min_{k \in \mathcal{T}} a_k)}
\end{equation}
Where $m$ is the total number of GPUs, and the denominator represents the total GPU-time available during the active scheduling window.

\section{Complexity Analysis}

% To justify the necessity of employing heuristic or meta-heuristic algorithms (e.g., Genetic Algorithms, Reinforcement Learning) rather than exact methods for large-scale instances (such as the 20,000 tasks in our dataset), we analyze the computational complexity of the proposed Heterogeneous GPU Scheduling Problem (HGSP).

% \begin{theorem}
The Heterogeneous GPU Scheduling Problem (HGSP) with the objective of minimizing Total Weighted Tardiness is $\mathcal{NP}$-hard.
% \end{theorem}

% \begin{proof_sketch}
We prove the $\mathcal{NP}$-hardness of the HGSP by \textbf{restriction}. We show that a special case of our general problem reduces to the \textit{Single Machine Total Weighted Tardiness Problem} ($1 || \sum w_j T_j$), which is a known $\mathcal{NP}$-hard problem.

Consider a restricted instance of the HGSP with the following constraints:
\begin{enumerate}
    \item \textbf{Single GPU:} The cluster consists of only one GPU ($m=1$).
    \item \textbf{Infinite Memory:} The memory capacity of the GPU is sufficiently large ($M_1 \to \infty$), or task memory demands are zero ($m_i = 0$), effectively removing the memory constraint.
    \item \textbf{Simultaneous Arrival:} All tasks arrive at time zero ($a_i = 0, \forall i \in \mathcal{T}$).
    \item \textbf{Homogeneous Processing:} Since there is only one GPU, the processing time $p_{ij}$ simplifies to a fixed processing time $p_i$ for each task.
\end{enumerate}

Under these restrictions, the decision variables reduce to finding a permutation (sequence) of tasks. The objective function remains:
\begin{equation}
    \text{Minimize } \sum_{i \in \mathcal{T}} w_i \cdot \max(0, C_i - d_i)
\end{equation}
where $C_i$ is determined solely by the sum of processing times of tasks preceding $i$ in the sequence plus $p_i$.

This restricted problem is exactly the \textbf{Single Machine Total Weighted Tardiness Problem}, denoted as $1 || \sum w_j T_j$ in Graham's notation.
It has been proven by Lawler (1977) and Lenstra et al. (1977) that $1 || \sum w_j T_j$ is $\mathcal{NP}$-hard in the strong sense.

Since a special case of the HGSP is $\mathcal{NP}$-hard, the general HGSP (which adds multiple heterogeneous machines, memory constraints, and release times) is at least as hard as this special case. Therefore, the HGSP is $\mathcal{NP}$-hard.
% \end{proof_sketch}

% Lawler, E. L. (1973).
% Optimal sequencing of a single machine subject to precedence constraints.
% Management Science, 19(5), 544–546.

% Lenstra, J. K., Rinnooy Kan, A. H. G., & Brucker, P. (1977).
% Complexity of machine scheduling problems.
% Annals of Discrete Mathematics, 1, 343–362.

\section{Algorithm Design}

Given the online nature of the problem, where tasks arrive dynamically ($a_i$) and full knowledge of future workloads is unavailable, we adopt a dynamic scheduling approach. The scheduler processes tasks based on their arrival order and assigns them to the heterogeneous GPU cluster using different strategies.

We propose three algorithms to schedule tasks to the Heterogeneous GPU cluster: a baseline First-In-First-Out (FIFO) approach, an improved Greedy strategy, and a meta-heuristic Simulated Annealing with Greedy assignment (SAGreedy) algorithm.

\subsection{Baseline Method: FIFO}

The First-In-First-Out (FIFO) scheduler serves as a simple baseline that processes tasks strictly in their arrival order. This approach is straightforward and provides a reference for comparing more sophisticated algorithms.

\subsubsection{Algorithm Logic}
At any scheduling decision point:
\begin{enumerate}
    \item \textbf{Sort}: Sort all tasks in ascending order of their arrival times $a_i$.
    \item \textbf{Assign}: For each task $i$ in order:
    \begin{itemize}
        \item Identify the set of valid GPUs $\mathcal{G}_{valid} = \{j \in \mathcal{G} \mid M_j \ge m_i\}$.
        \item If $\mathcal{G}_{valid} = \emptyset$, skip the task (resource unavailable).
        \item Otherwise, assign task $i$ to the GPU $j^* \in \mathcal{G}_{valid}$ that minimizes the start time.
    \end{itemize}
\end{enumerate}

The start time on GPU $j$ is calculated as $s_{ij} = \max(a_i, \text{avail}_j)$, where $\text{avail}_j$ is the time GPU $j$ finishes its currently assigned tasks. The completion time is $c_{ij} = s_{ij} + \frac{L_i}{v_j}$.

\subsubsection{Complexity Analysis}
Let $N$ be the number of tasks and $M$ be the number of GPUs. The complexity is $O(N \log N + N \cdot M)$ for sorting and GPU assignment.

\subsection{Improved Heuristic: Greedy (EFT)}

The Greedy scheduler improves upon FIFO by considering both arrival time and GPU heterogeneity when making scheduling decisions. It uses an Earliest Finish Time (EFT) strategy that accounts for GPU computing speeds.

\subsubsection{Algorithm Logic}
At any scheduling decision point:
\begin{enumerate}
    \item \textbf{Sort}: Sort all tasks in ascending order of their arrival times $a_i$.
    \item \textbf{Assign}: For each task $i$ in order:
    \begin{itemize}
        \item Identify the set of valid GPUs $\mathcal{G}_{valid} = \{j \in \mathcal{G} \mid M_j \ge m_i\}$.
        \item If $\mathcal{G}_{valid} = \emptyset$, skip the task.
        \item Otherwise, assign task $i$ to the GPU $j^* \in \mathcal{G}_{valid}$ that minimizes the completion time $c_{ij} = \max(a_i, \text{avail}_j) + \frac{L_i}{v_j}$.
    \end{itemize}
\end{enumerate}

\textbf{Key Difference from FIFO}: While FIFO selects the GPU with the earliest start time, Greedy considers the execution time on different GPUs (varying $v_j$) and selects the GPU that minimizes the actual completion time.

\subsubsection{Complexity Analysis}
The complexity remains $O(N \log N + N \cdot M)$, but the EFT strategy better exploits GPU heterogeneity for improved performance.

\subsection{Meta-Heuristic: SAGreedy}

To address the NP-Hardness of minimizing Total Weighted Tardiness (as proven in Section II), we propose a Simulated Annealing with Greedy GPU assignment (SAGreedy) algorithm. This meta-heuristic explores the global solution space of task permutations while using efficient greedy GPU assignment for evaluation.

\subsubsection{Algorithm Overview}
\begin{enumerate}
    \item \textbf{Initialization}: Start with the Greedy solution as the initial state.
    \item \textbf{Temperature Schedule}: Use exponential cooling $T_{k+1} = \alpha \cdot T_k$ where $\alpha \in (0, 1)$.
    \item \textbf{Neighbor Generation}: Generate new task ordering by prioritizing tardy tasks based on current temperature.
    \item \textbf{Acceptance Criterion}: Accept worse solutions with probability $e^{-\Delta/T}$ to escape local optima.
    \item \textbf{GPU Assignment}: For each task ordering, use greedy GPU assignment (EFT) for evaluation.
\end{enumerate}

\subsubsection{Fitness Function}
The fitness function corresponds directly to the minimization objective defined in Eq. (1). For a given task ordering with greedy GPU assignment, the fitness value (to be minimized) is:
\begin{equation}
    Fitness = \sum_{i \in \mathcal{T}} w_i \cdot \max(0, c_i - d_i)
\end{equation}

\subsubsection{Priority-Based Neighbor Generation}

A key innovation in SAGreedy is the temperature-dependent neighbor generation strategy that explicitly targets tardy tasks:
\begin{itemize}
    \item \textbf{High Temperature}: Prioritize a larger fraction of tardy tasks, enabling more exploration of the solution space.
    \item \textbf{Low Temperature}: Prioritize fewer tardy tasks, focusing on exploitation of promising solutions.
\end{itemize}

\paragraph{Step-by-Step Neighbor Generation}

Given a current task ordering $\pi$, the neighbor generation process works as follows:

\begin{enumerate}
    \item \textbf{Identify Tardy Tasks}: Compute the set $\tau = \{i \in \mathcal{T} \mid c_i > d_i\}$ of tasks that miss their deadlines under the current ordering, where $c_i$ is the completion time and $d_i$ is the deadline.

    \item \textbf{Calculate Priority Ratio}: The priority ratio $\rho \in [0.1, 0.8]$ varies with temperature:
\begin{equation}
    \rho = \min(0.8, \max(0.1, \frac{T - T_{min}}{T_{initial} - T_{min}}))
\end{equation}
When $T = T_{initial}$, $\rho = 0.8$ (80\% of tardy tasks prioritized). When $T = T_{min}$, $\rho = 0.1$ (10\% of tardy tasks prioritized).

    \item \textbf{Select Critical Tasks}: Select the top $k = \lfloor \rho \cdot |\tau| \rfloor$ tardy tasks with the largest weighted tardiness $w_i \cdot (c_i - d_i)$. These are the most problematic tasks that need repositioning.

    \item \textbf{Generate New Ordering}: Create a new task ordering $\pi'$ by:
    \begin{itemize}
        \item Placing the $k$ selected tardy tasks at the front of the queue, sorted by their weighted tardiness (descending).
        \item Placing all remaining tasks in their original relative order.
    \end{itemize}
\end{enumerate}

\paragraph{Intuition}

By moving the most tardy tasks earlier in the queue, we give them higher priority for GPU allocation. This directly targets the objective function (minimizing weighted tardiness) since tasks scheduled earlier tend to complete earlier. The temperature-controlled ratio $\rho$ balances exploration (high $\rho$ = many tasks moved) versus exploitation (low $\rho$ = only the worst tasks moved).

\subsubsection{Complexity Analysis}
Let $N$ be the number of tasks, $M$ be the number of GPUs, $K$ be the maximum iterations, and $T_{initial}$ be the initial temperature.
\begin{itemize}
    \item \textbf{Fitness Evaluation}: For each candidate solution, greedy assignment involves $O(N \cdot M)$ operations.
    \item \textbf{Total Complexity}: $O(K \cdot N \cdot M)$ for the complete annealing process.
\end{itemize}

Although computationally more expensive than FIFO and Greedy ($O(N \log N + N \cdot M)$), SAGreedy allows for escaping local optima by explicitly considering weighted tardiness in the optimization objective.





\section{Experimental Evaluation}

\subsection{Experimental Setup}

\subsubsection{Dataset Description}
We evaluate the proposed algorithms on four datasets with varying characteristics:
\begin{itemize}
    \item \textbf{tasks1}: Course-provided dataset with 1000 tasks, medium load level.
    \item \textbf{tasks2}: Course-provided dataset with 2000 tasks, medium load level.
    \item \textbf{tasks3}: Self-generated dataset with 1000 tasks, high load level (tighter deadlines).
    \item \textbf{tasks4}: Self-generated dataset with 2000 tasks, extreme load level (very tight deadlines).
\end{itemize}

Each task is characterized by workload ($L_i$), memory demand ($m_i$), deadline ($d_i$), weight ($w_i$), and arrival time ($a_i$). The datasets feature heterogeneous task requirements and dynamic arrival patterns.

\subsubsection{Cluster Configuration}
We use three cluster sizes to evaluate scalability:
\begin{itemize}
    \item \textbf{Small}: 3 GPUs (1 A100, 1 A30, 1 L40)
    \item \textbf{Medium}: 6 GPUs (2 of each model)
    \item \textbf{Large}: 9 GPUs (3 of each model)
\end{itemize}

The GPU specifications are based on real NVIDIA hardware: A100 (80GB, 57.0x scaling), A30 (24GB, 30.0x baseline), and L40 (48GB, 55.5x scaling). This heterogeneity forces multi-GPU collaboration as no single GPU can handle all tasks independently.

\subsubsection{Algorithm Parameters}
For the SAGreedy meta-heuristic, we use the following parameters:
\begin{itemize}
    \item Initial temperature: $T_{initial} = 1000$
    \item Cooling rate: $\alpha = 0.95$
    \item Maximum iterations: $K = 100$
    \item Priority ratio range: $\rho \in [0.1, 0.8]$
\end{itemize}

\subsection{Results and Discussion}

Table \ref{tab:comprehensive_results} presents a comprehensive performance comparison of the three algorithms across four datasets with varying load levels and cluster configurations.

% Comprehensive table with all datasets
\begin{table*}[htbp]
\caption{Comprehensive algorithm performance comparison across all datasets}
\label{tab:comprehensive_results}
\centering
\small
\begin{tabular}{l|ll|ll|ll|ll}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c|}{\textbf{FIFO}} & \multicolumn{2}{c|}{\textbf{Greedy}} & \multicolumn{2}{c|}{\textbf{SAGreedy}} & \multicolumn{2}{c}{\textbf{Best}} \\
\cline{2-9}
 & WT & DMR (\%) & WT & DMR (\%) & WT & DMR (\%) & WT & DMR (\%) \\
\hline
\textbf{tasks1} (small) & 524 & 5.7\% & 503 & 5.2\% & \textbf{393} & \textbf{4.9\%} & \textbf{-25\%} & \textbf{-14\%} \\
\textbf{tasks2} (medium) & 140 & 1.9\% & \textbf{0.35} & \textbf{0.1\%} & \textbf{0.35} & \textbf{0.1\%} & \textbf{-99.7\%} & \textbf{-95\%} \\
\textbf{tasks3} (small, high) & 4.40M & 99.5\% & 4.40M & 99.6\% & \textbf{3.96M} & \textbf{56.7\%} & \textbf{-10\%} & \textbf{-43\%} \\
\textbf{tasks4} (large, extreme) & 2.26M & 99.1\% & 2.26M & 99.1\% & \textbf{2.23M} & \textbf{91.3\%} & \textbf{-1.3\%} & \textbf{-7.8\%} \\
\hline
\multicolumn{9}{l}{\footnotesize \textit{WT: Weighted Tardiness, DMR: Deadline Miss Rate. Best values shown in bold.}} \\
\multicolumn{9}{l}{\footnotesize \textit{Best columns show SAGreedy improvement over FIFO.}} \\
\end{tabular}
\end{table*}

\subsubsection{Key Observations}

\textbf{1. Consistent Weighted Tardiness Improvement:}
SAGreedy achieves the lowest weighted tardiness (WT) across all four datasets. On the high-load tasks3 dataset, SAGreedy reduces WT by 10\% and deadline miss rate by 43 percentage points compared to FIFO. On the moderate-load tasks1 dataset, SAGreedy reduces WT by 25\% compared to FIFO, validating the effectiveness of the simulated annealing approach.

\textbf{2. Deadline Miss Reduction Under Pressure:}
On datasets with moderate load (tasks1, tasks2), all algorithms maintain reasonable deadline miss rates below 6\%. However, under high load (tasks3) and extreme load (tasks4), SAGreedy significantly outperforms the baselines. On tasks3, SAGreedy reduces the deadline miss rate from 99.5\% to 56.7\%---a 43\% absolute improvement by explicitly prioritizing tardy tasks.

\textbf{3. Load-Dependent Performance:}
The performance gap between algorithms varies with dataset characteristics. On easy instances (tasks2), all methods achieve near-optimal solutions with minimal deadline misses. On hard instances (tasks3, tasks4), the advantage of SAGreedy becomes more pronounced, demonstrating the value of meta-heuristic search for complex scheduling scenarios.

\textbf{4. Greedy vs. SAGreedy Trade-off:}
While Greedy (EFT) performs competitively on easy datasets, SAGreedy provides consistent improvements across all load levels by explicitly optimizing the weighted tardiness objective through task reordering.

\subsubsection{Visual Analysis: Complete Task Schedules}

Figure \ref{fig:gantt_all_tasks} presents complete Gantt chart visualizations for all 1000 tasks in the tasks1 dataset across all three algorithms. These charts show the entire scheduling horizon, revealing how each algorithm manages task assignments, deadlines, and resource utilization over time.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_FIFO_all.png}
\hfill
\includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_Greedy_all.png}
\hfill
\includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_SAGreedy_all.png}
\caption{Complete Gantt charts for tasks1 (small cluster) showing all 1000 tasks. Left: FIFO, Center: Greedy, Right: SAGreedy. Red bars indicate deadline misses, green bars indicate on-time completion.}
\label{fig:gantt_all_tasks}
\end{figure*}

Detailed Gantt charts and performance visualizations for all dataset and cluster configurations are available in the project repository. Refer to Appendix~\ref{appendix:visualization_paths} for the complete directory structure and file locations.

\section{Conclusion}

This paper addresses the Heterogeneous GPU Scheduling Problem (HGSP), an NP-hard optimization problem with practical significance in cloud computing and high-performance computing environments. We formalized the problem with weighted tardiness minimization as the primary objective and proposed three scheduling algorithms:

\textbf{FIFO} provides a simple baseline that processes tasks in arrival order. \textbf{Greedy (EFT)} improves upon FIFO by considering GPU heterogeneity through earliest finish time assignment. \textbf{SAGreedy} combines simulated annealing for task ordering with greedy GPU assignment to explicitly optimize weighted tardiness.

Experimental evaluation on four datasets with varying load levels demonstrates that:
\begin{enumerate}
    \item SAGreedy consistently achieves the lowest weighted tardiness, with up to 25\% improvement over FIFO on moderate-load datasets.
    \item Under extreme load, SAGreedy reduces deadline miss rate by 7.8 percentage points compared to baselines.
    \item GPU utilization alone is an insufficient metric; intelligent scheduling that considers deadlines and priorities significantly improves system performance.
\end{enumerate}

The results validate that meta-heuristic approaches like simulated annealing can effectively navigate the complex solution space of heterogeneous scheduling problems, even with dynamic task arrivals. Future work may explore reinforcement learning for online adaptation and multi-objective optimization considering energy consumption and fairness.

\section{LLM Inference GPU Scheduling}

As an extension to the general heterogeneous GPU scheduling problem, we consider the specialized case of scheduling LLM inference requests on GPU clusters. LLM inference has unique characteristics that distinguish it from general computational workloads, particularly the two-phase execution model (prefill and decode) and the use of tensor parallelism for multi-GPU execution.

\subsection{Problem Formulation}

\subsubsection{LLM Inference Characteristics}

Unlike traditional GPU workloads, LLM inference operates in two distinct phases:

\begin{itemize}
    \item \textbf{Prefill Phase (Compute-Bound)}: Processes the entire input prompt in parallel. This phase is compute-intensive with high arithmetic intensity, processing tokens at $\sim$1000--2000 tokens/s. It generates the key-value (KV) cache required for decoding.

    \item \textbf{Decode Phase (Memory-Bound)}: Generates output tokens auto-regressively using the cached keys and values. This phase is memory bandwidth-limited, processing tokens at $\sim$100--500 tokens/s (single request), but can benefit significantly from continuous batching where multiple decode requests share GPU resources.
\end{itemize}

\subsubsection{Task Model}

The LLM task model differs from the general model in several key aspects:

\textbf{Task Parameters:}
\begin{itemize}
    \item $M_i$: Model name (e.g., ``Qwen3'', ``DeepSeek-R1'', ``Llama3-70B'')
    \item $L_i$: Number of tokens to process (prefill or decode)
    \item $\phi_i \in \{\text{PREFILL}, \text{DECODE}\}$: Execution phase
    \item $m_i$: Total memory requirement (GB) for the model
    \item $\tau_i$: Tensor parallelism degree (number of GPUs required)
    \item $w_i$: Task priority weight
    \item $a_i$: Arrival time
\end{itemize}

\textbf{Derived Parameters:}
\begin{itemize}
    \item $p_{ij}^{\phi} = \frac{L_i}{\lambda_{j}^{\phi}}$: Execution time on GPU $j$ in phase $\phi$, where $\lambda_{j}^{\phi}$ is the token throughput (tokens/s).
    \item For prefill: $\lambda_{j}^{\text{PREFILL}} \approx 1000$ tokens/s
    \item For decode: $\lambda_{j}^{\text{DECODE}} \approx 5000$ tokens/s
\end{itemize}

\textbf{Constraints:}
\begin{itemize}
    \item \textbf{Multi-GPU Allocation}: Each task requires exactly $\tau_i$ GPUs for tensor parallelism
    \item \textbf{Memory Capacity}: $\frac{m_i}{\tau_i} \le M_j$ for each allocated GPU $j$
    \item \textbf{No Deadlines}: LLM inference tasks optimize for response time rather than meeting hard deadlines
\end{itemize}

\subsubsection{Objective Function}

The primary objective is to minimize the \textbf{Total Weighted Waiting Time}:
\begin{equation}
    \text{Minimize } Z = \sum_{i \in \mathcal{T}} w_i \cdot (c_i - a_i)
\end{equation}
where $c_i$ is the completion time and $a_i$ is the arrival time.

Secondary metrics include:
\begin{itemize}
    \item Average Waiting Time: $\frac{1}{n} \sum_{i \in \mathcal{T}} (c_i - a_i)$
    \item Makespan: $C_{\max} = \max_{i \in \mathcal{T}} c_i$
    \item Completion Rate: $\frac{|\{i : c_i \le T_{\max}\}|}{n}$
\end{itemize}

\subsection{Scheduling Algorithm}

We implement the Weighted Shortest Remaining Processing Time (WSRPT) algorithm for LLM inference scheduling.

\subsubsection{WSRPT (Weighted Shortest Remaining Processing Time)}

WSRPT combines task priority and execution duration to optimize the weighted waiting time objective. The algorithm maintains a priority queue of waiting tasks, prioritized by their weight-to-duration ratio:

\begin{equation}
    \text{Priority}(i) = \frac{w_i}{p_i^{\text{prefill}} + p_i^{\text{decode}}}
\end{equation}

where $p_i^{\text{prefill}}$ and $p_i^{\text{decode}}$ are the estimated execution times for prefill and decode phases, respectively. At each scheduling decision, WSRPT selects the task with the highest priority ratio and allocates the required GPU resources ($\tau_i$ GPUs with sufficient memory).

\subsection{Experimental Evaluation}

\subsubsection{Dataset Description}

Since LLM inference workloads differ significantly from traditional GPU tasks, we design a custom synthetic workload that highlights algorithmic differences:

\textbf{Discriminating Workload Design:}
\begin{itemize}
    \item 20 tasks arrive simultaneously at $t=0$
    \item Cluster: 16 H100 GPUs (can run 8 concurrent tasks with $\tau=2$)
    \item First 8 tasks: Low weight ($w=1$), long duration (2000 tokens)
    \item Next 12 tasks: High weight ($w=10$), short duration (500 tokens)
\end{itemize}

This design forces a scheduling decision: only 8 of 20 tasks can run initially. The choice of which 8 tasks to start determines the weighted waiting time.

\textbf{GPU and Model Specifications:}
\begin{itemize}
    \item GPU: H100 80GB (3350 GB/s bandwidth, 989 TFLOPS compute)
    \item Model: Qwen3 (30B parameters, requires 15GB per GPU with $\tau=2$)
\end{itemize}

\subsubsection{Results}

Table \ref{tab:llm_discriminating_results} presents the performance comparison between FIFO and WSRPT on the discriminating workload.

\begin{table}[htbp]
\caption{Algorithm performance comparison on discriminating LLM workload (16 H100 GPUs)}
\label{tab:llm_discriminating_results}
\centering
\begin{tabular}{l|ccc}
\hline
Algorithm & Weighted Wait & Avg Wait & Completed \\
\hline
FIFO & 336.400 & 2.402 & 20 \\
WSRPT & 108.040 & 1.802 & 20 \\
\hline
\textbf{Improvement} & \textbf{-67.9\%} & \textbf{-25.0\%} & -- \\
\hline
\end{tabular}
\end{table}

\subsubsection{Key Observations}

\textbf{1. Significant Weighted Waiting Time Reduction:}
WSRPT achieves a 67.9\% reduction in weighted waiting time compared to FIFO. This demonstrates that considering both task priority and duration is crucial for LLM inference scheduling.

\textbf{2. FIFO Inefficiency:}
FIFO selects the first 8 tasks by arrival order (all low-weight, long-duration), causing high-weight tasks to wait $\sim$2 seconds. WSRPT prioritizes high-weight, short-duration tasks, completing them quickly before starting low-weight tasks.

\textbf{3. Improved Average Responsiveness:}
WSRPT reduces average waiting time by 25.0\% compared to FIFO (1.802s vs. 2.402s), confirming that shorter tasks should be prioritized to improve system responsiveness.

\subsection{Discrete-Time Event Simulator}

The LLM inference scheduling is evaluated using a discrete-time event simulator that models task arrivals, GPU allocations, and completions at fine-grained time steps.

\subsubsection{Simulation Granularity}

The simulator operates with a time step of $\Delta t = 0.01$ seconds (10 milliseconds), providing sufficient granularity to accurately capture task dynamics while maintaining computational efficiency. At each time step, the simulator processes three types of events:

\begin{enumerate}
    \item \textbf{Task Completions}: Tasks finishing at the current time step complete, freeing their allocated GPU resources.
    \item \textbf{Task Arrivals}: New tasks arriving at the current time step are added to the waiting queue.
    \item \textbf{Scheduling Decisions}: The scheduler selects tasks from the waiting queue and allocates GPU resources based on the chosen algorithm.
\end{enumerate}

\subsubsection{Simulation Procedure}

The simulation proceeds as follows:
\begin{enumerate}
    \item Initialize the cluster with available GPU resources.
    \item For each time step $t$ from 0 to $T_{\max}$:
    \begin{itemize}
        \item Complete tasks whose finish time equals $t$, freeing their GPU allocations.
        \item Add tasks whose arrival time equals $t$ to the waiting queue.
        \item While GPU resources are available and the queue is non-empty:
        \begin{itemize}
            \item Select a task from the queue according to the scheduling policy.
            \item Allocate $\tau_i$ GPUs with sufficient memory capacity.
            \item Set the task's start time to $t$ and compute completion time.
        \end{itemize}
    \end{itemize}
    \item Terminate when all tasks have completed.
\end{enumerate}

This discrete-time approach ensures that scheduling decisions are made when \textbf{multiple tasks are competing for resources}, making algorithmic differences clearly visible. In contrast to event-driven simulators that schedule tasks immediately upon arrival (resulting in queue size of 1), our approach allows priority-based algorithms to demonstrate their effectiveness by reordering waiting tasks.

\appendix
\section{Visualization Repository Paths}
\label{appendix:visualization_paths}

This appendix provides the complete directory structure for all experimental visualizations, including Gantt charts, algorithm comparison plots, and GPU utilization figures.

\subsection{Directory Structure}

All visualization files are organized under the \texttt{results/} directory with the following naming convention:
\begin{center}
\footnotesize\texttt{results/<dataset>\_<cluster\_size>\_tasks<N>/figures/}\normalsize
\end{center}
where:
\begin{itemize}
    \item \texttt{<dataset>}: Dataset identifier (sf40 = scaling factor 40)
    \item \texttt{<cluster\_size>}: small, medium, or large
    \item \texttt{<N>}: Task dataset ID (1, 2, 3, or 4)
\end{itemize}

\subsection{Complete Path Listing}

\subsubsection{tasks1 (1000 tasks, medium load)}
\begin{itemize}
    \item Small cluster:\\
          \path{results/sf40_small_tasks1/figures/}
    \item Medium cluster:\\
          \path{results/sf40_medium_tasks1/figures/}
    \item Large cluster:\\
          \path{results/sf40_large_tasks1/figures/}
\end{itemize}

\subsubsection{tasks2 (2000 tasks, medium load)}
\begin{itemize}
    \item Small cluster:\\
          \path{results/sf40_small_tasks2/figures/}
    \item Medium cluster:\\
          \path{results/sf40_medium_tasks2/figures/}
    \item Large cluster:\\
          \path{results/sf40_large_tasks2/figures/}
\end{itemize}

\subsubsection{tasks3 (1000 tasks, high load)}
\begin{itemize}
    \item Small cluster:\\
          \path{results/sf40_small_tasks3/figures/}
    \item Medium cluster:\\
          \path{results/sf40_medium_tasks3/figures/}
    \item Large cluster:\\
          \path{results/sf40_large_tasks3/figures/}
\end{itemize}

\subsubsection{tasks4 (2000 tasks, extreme load)}
\begin{itemize}
    \item Small cluster:\\
          \path{results/sf40_small_tasks4/figures/}
    \item Medium cluster:\\
          \path{results/sf40_medium_tasks4/figures/}
    \item Large cluster:\\
          \path{results/sf40_large_tasks4/figures/}
\end{itemize}

\subsection{Available Visualizations}

Each directory contains the following types of visualization files:

\footnotesize
\begin{itemize}
    \item \textbf{Gantt Charts}:
    \begin{itemize}
        \item \texttt{<dataset>\_<size>\_gantt\_FIFO\_first50.png}
        \item \texttt{<dataset>\_<size>\_gantt\_Greedy\_first50.png}
        \item \texttt{<dataset>\_<size>\_gantt\_SAGreedy\_first50.png}
        \item \texttt{<dataset>\_<size>\_gantt\_FIFO\_all.png}
        \item \texttt{<dataset>\_<size>\_gantt\_Greedy\_all.png}
        \item \texttt{<dataset>\_<size>\_gantt\_SAGreedy\_all.png}
    \end{itemize}
    \item \textbf{Comparison Plots}:
    \begin{itemize}
        \item \texttt{<dataset>\_<size>\_comparison.png}
    \end{itemize}
    \item \textbf{Utilization Figures}:
    \begin{itemize}
        \item \texttt{<dataset>\_<size>\_gpu\_utilization.png}
    \end{itemize}
\end{itemize}
\normalsize

% \section{Additional Experimental Results}

% This appendix provides comprehensive experimental results across all dataset and cluster size combinations. Tables IV-X present detailed performance metrics for configurations not shown in the main text. Figures 3-26 display Gantt chart visualizations for all algorithms.

% \subsection{Additional Performance Metrics}

% Tables IV-X show algorithm performance on remaining dataset/cluster combinations.

% % Table for tasks1 (large cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks1 (large cluster)}
% \label{tab:tasks1_large_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 14938 & 6.07 & 15 & 1.5\% & 83.22 & 2496 & 27.0\% & 8.3\% \\
% Greedy & 13119 & 5.34 & 0 & 0.0\% & 0.00 & 2496 & 23.8\% & 4.9\% \\
% SAGreedy & 13119 & 5.34 & 0 & 0.0\% & 0.00 & 2496 & 23.8\% & 4.9\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks2 (small cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks2 (small cluster)}
% \label{tab:tasks2_small_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 21785 & 8.67 & 52 & 5.2\% & 524.5 & 2495 & 84.5\% & 31.4\% \\
% Greedy & 22342 & 8.92 & 53 & 5.3\% & 504.3 & 2496 & 78.9\% & 25.6\% \\
% SAGreedy & 22418 & 8.97 & 54 & 5.4\% & 454.8 & 2496 & 78.9\% & 25.7\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks3 (small cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks3 (small cluster)}
% \label{tab:tasks3_small_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 4.48M & 995 & 1493 & 99.5\% & 4.40M & 3506 & 99.8\% & 38.5\% \\
% Greedy & 4.47M & 995 & 1494 & 99.6\% & 4.40M & 3501 & 99.9\% & 38.1\% \\
% SAGreedy & 4.02M & 922 & 851 & 56.7\% & 3.96M & 3797 & 92.2\% & 35.3\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks3 (medium cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks3 (medium cluster)}
% \label{tab:tasks3_medium_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 554470 & 123.3 & 1430 & 95.3\% & 480572 & 1757 & 99.6\% & 38.5\% \\
% Greedy & 553083 & 123.0 & 1428 & 95.2\% & 479169 & 1754 & 99.7\% & 38.3\% \\
% SAGreedy & 484357 & 112.0 & 1318 & 87.9\% & 411817 & 1768 & 98.8\% & 37.6\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks3 (large cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks3 (large cluster)}
% \label{tab:tasks3_large_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 35691 & 7.94 & 51 & 3.4\% & 221.2 & 1512 & 78.2\% & 30.5\% \\
% Greedy & 35410 & 7.89 & 33 & 2.2\% & 171.2 & 1503 & 70.0\% & 21.4\% \\
% SAGreedy & 35669 & 7.95 & 27 & 1.8\% & 75.21 & 1503 & 70.1\% & 21.5\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks4 (small cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks4 (small cluster)}
% \label{tab:tasks4_small_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 13.12M & 2149 & 1996 & 99.8\% & 13.04M & 5319 & 100.0\% & 38.1\% \\
% Greedy & 13.12M & 2149 & 1995 & 99.8\% & 13.04M & 5320 & 99.9\% & 37.6\% \\
% SAGreedy & 12.92M & 2135 & 1957 & 97.9\% & 12.84M & 5334 & 99.7\% & 37.8\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks4 (medium cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks4 (medium cluster)}
% \label{tab:tasks4_medium_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 5.03M & 823.8 & 1989 & 99.5\% & 4.95M & 2664 & 99.8\% & 38.3\% \\
% Greedy & 5.03M & 823.8 & 1991 & 99.6\% & 4.95M & 2665 & 99.7\% & 38.0\% \\
% SAGreedy & 4.95M & 815.9 & 1884 & 94.2\% & 4.88M & 2683 & 99.0\% & 37.2\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% \subsection{Gantt Charts}

% Figures 3-26 show Gantt chart visualizations for all algorithms across different dataset and cluster configurations. Each row displays FIFO (left), Greedy (center), and SAGreedy (right) results.

% % tasks1 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_SAGreedy_first50.png}
% \caption{tasks1 (small): First 50 tasks.}
% \label{fig:gantt_tasks1_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_SAGreedy_all.png}
% \caption{tasks1 (small): All tasks.}
% \label{fig:gantt_tasks1_small_all}
% \end{figure}

% % tasks1 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_SAGreedy_first50.png}
% \caption{tasks1 (medium): First 50 tasks.}
% \label{fig:gantt_tasks1_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_SAGreedy_all.png}
% \caption{tasks1 (medium): All tasks.}
% \label{fig:gantt_tasks1_medium_all}
% \end{figure}

% % tasks1 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_SAGreedy_first50.png}
% \caption{tasks1 (large): First 50 tasks.}
% \label{fig:gantt_tasks1_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_SAGreedy_all.png}
% \caption{tasks1 (large): All tasks.}
% \label{fig:gantt_tasks1_large_all}
% \end{figure}

% % tasks2 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_SAGreedy_first50.png}
% \caption{tasks2 (small): First 50 tasks.}
% \label{fig:gantt_tasks2_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_SAGreedy_all.png}
% \caption{tasks2 (small): All tasks.}
% \label{fig:gantt_tasks2_small_all}
% \end{figure}

% % tasks2 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_SAGreedy_first50.png}
% \caption{tasks2 (medium): First 50 tasks.}
% \label{fig:gantt_tasks2_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_SAGreedy_all.png}
% \caption{tasks2 (medium): All tasks.}
% \label{fig:gantt_tasks2_medium_all}
% \end{figure}

% % tasks2 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_SAGreedy_first50.png}
% \caption{tasks2 (large): First 50 tasks.}
% \label{fig:gantt_tasks2_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_SAGreedy_all.png}
% \caption{tasks2 (large): All tasks.}
% \label{fig:gantt_tasks2_large_all}
% \end{figure}

% % tasks3 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_SAGreedy_first50.png}
% \caption{tasks3 (small): First 50 tasks.}
% \label{fig:gantt_tasks3_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_SAGreedy_all.png}
% \caption{tasks3 (small): All tasks.}
% \label{fig:gantt_tasks3_small_all}
% \end{figure}

% % tasks3 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_SAGreedy_first50.png}
% \caption{tasks3 (medium): First 50 tasks.}
% \label{fig:gantt_tasks3_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_SAGreedy_all.png}
% \caption{tasks3 (medium): All tasks.}
% \label{fig:gantt_tasks3_medium_all}
% \end{figure}

% % tasks3 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_SAGreedy_first50.png}
% \caption{tasks3 (large): First 50 tasks.}
% \label{fig:gantt_tasks3_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_SAGreedy_all.png}
% \caption{tasks3 (large): All tasks.}
% \label{fig:gantt_tasks3_large_all}
% \end{figure}

% % tasks4 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_SAGreedy_first50.png}
% \caption{tasks4 (small): First 50 tasks.}
% \label{fig:gantt_tasks4_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_SAGreedy_all.png}
% \caption{tasks4 (small): All tasks.}
% \label{fig:gantt_tasks4_small_all}
% \end{figure}

% % tasks4 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_SAGreedy_first50.png}
% \caption{tasks4 (medium): First 50 tasks.}
% \label{fig:gantt_tasks4_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_SAGreedy_all.png}
% \caption{tasks4 (medium): All tasks.}
% \label{fig:gantt_tasks4_medium_all}
% \end{figure}

% % tasks4 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_SAGreedy_first50.png}
% \caption{tasks4 (large): First 50 tasks.}
% \label{fig:gantt_tasks4_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_SAGreedy_all.png}
% \caption{tasks4 (large): All tasks.}
% \label{fig:gantt_tasks4_large_all}
% \end{figure}

\end{document}


