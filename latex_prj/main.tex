\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}

\usepackage{algorithm} % 需要添加这个包来支持 algorithm 环境
\usepackage{algorithmic}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{Heterogeneous GPU Scheduling}
% \author{IEEE Publication Technology Department


\maketitle

\section{Formal Problem Description: Simplified Heterogeneous GPU Scheduling}

\subsection{Sets and Indices}
\begin{itemize}
    \item $\mathcal{T} = \{1, 2, \dots, n\}$: Set of tasks .
    \item $\mathcal{G} = \{1, 2, \dots, m\}$: Set of available GPUs in the cluster.
    \item $i, k \in \mathcal{T}$: Indices for tasks.
    \item $j \in \mathcal{G}$: Index for GPUs.
\end{itemize}

\subsection{Parameters}
\textbf{Task Model (from Dataset):}
\begin{itemize}
    \item $L_i$: Workload of task $i$ (Floating point operations or relative units).
    \item $m_i$: Memory demand of task $i$ (GB).
    \item $d_i$: Deadline of task $i$ (Time unit).
    \item $w_i$: Weight/Priority of task $i$.
    \item $a_i$: Arrival time of task $i$.
\end{itemize}

\textbf{Resource Model (GPU Cluster):}
\begin{itemize}
    \item $v_j$: Computing speed of GPU $j$ (Workload per time unit).
    \item $M_j$: Memory capacity of GPU $j$ (GB).
\end{itemize}

\textbf{Derived Parameter:}
\begin{itemize}
    \item $p_{ij} = \frac{L_i}{v_j}$: Execution time of task $i$ if assigned to GPU $j$.
\end{itemize}

\subsection{Decision Variables}
\begin{itemize}
    \item $x_{ij} \in \{0, 1\}$: Binary variable. equals 1 if task $i$ is assigned to GPU $j$, 0 otherwise.
    \item $s_i \ge 0$: Start time of task $i$.
    \item $c_i \ge 0$: Completion time of task $i$.
    \item $y_{ik} \in \{0, 1\}$: Binary sequence variable (for tasks on the same GPU). Equals 1 if task $i$ precedes task $k$, 0 otherwise.
\end{itemize}

\subsection{Mathematical Formulation}

\paragraph{\textbf{Objective Function}}
Minimize the Total Weighted Tardiness. This balances the priority ($w_i$) and the urgency (meeting $d_i$).
\begin{equation}
    \text{Minimize } Z = \sum_{i \in \mathcal{T}} w_i \cdot \max(0, c_i - d_i)
\end{equation}

\paragraph{Constraints}

1. Assignment Constraint:
Each task must be assigned to exactly one GPU.
\begin{equation}
    \sum_{j \in \mathcal{G}} x_{ij} = 1, \quad \forall i \in \mathcal{T}
\end{equation}

2. Memory Constraint:
A task can only be assigned to a GPU if the GPU's memory capacity is sufficient.
\begin{equation}
    x_{ij} \cdot m_i \le M_j, \quad \forall i \in \mathcal{T}, \forall j \in \mathcal{G}
\end{equation}

3. Timing Constraints:
The completion time is the start time plus the execution time on the assigned GPU.
\begin{equation}
    c_i = s_i + \sum_{j \in \mathcal{G}} x_{ij} p_{ij}, \quad \forall i \in \mathcal{T}
\end{equation}
A task cannot start before its arrival time.
\begin{equation}
    s_i \ge a_i, \quad \forall i \in \mathcal{T}
\end{equation}

4. Non-overlapping Constraint (Disjunctive):
If two tasks $i$ and $k$ are assigned to the same GPU, they cannot overlap in time. ($H$ is a sufficiently large positive number).
\begin{equation}
\begin{aligned}
    s_i + \sum_{j \in \mathcal{G}} x_{ij} p_{ij} &\le s_k + H(3 - x_{ij} - x_{kj} - y_{ik}) \\
    s_k + \sum_{j \in \mathcal{G}} x_{kj} p_{kj} &\le s_i + H(3 - x_{ij} - x_{kj} - (1-y_{ik}))
\end{aligned}
\end{equation}
This ensures that if both are on GPU $j$, either $i$ finishes before $k$ starts or $k$ finishes before $i$ starts.

\subsection{Performance Metrics}

Based on the decision variables and parameters defined above, the following metrics are used to evaluate the scheduling performance:

\subsubsection{Total Weighted Completion Time (TWCT)}
This metric represents the overall value-weighted responsiveness of the system. Lower is better.
\begin{equation}
    \text{TWCT} = \sum_{i \in \mathcal{T}} w_i \cdot (c_i - a_i)
\end{equation}
% \textit{Note: If the focus is on the duration from arrival, Weighted Turnaround Time ($\sum w_i (C_i - a_i)$) can also be used.}

\subsubsection{Average Completion Time (ACT)}
The average time at which tasks finish execution. Lower is better.
\begin{equation}
    \text{ACT} = \frac{1}{n} \sum_{i \in \mathcal{T}} (c_i - a_i)
\end{equation}
% \textit{Alternatively, Average Turnaround Time (ATT) describes the average time a task spends in the system:}
% \begin{equation}
%     \text{ATT} = \frac{1}{n} \sum_{i \in \mathcal{T}} (C_i - a_i)
% \end{equation}

\subsubsection{Deadline Miss Count (DMC)}
The total number of tasks that failed to complete before their deadline. Lower is better.
Let $\mathbb{I}(\cdot)$ be an indicator function that equals 1 if the condition is true, and 0 otherwise.
\begin{equation}
    \text{DMC} = \sum_{i \in \mathcal{T}} \mathbb{I}(C_i > d_i)
\end{equation}

\subsubsection{Deadline Miss Rate (DMR)}
The proportion of tasks that failed to complete before their deadline. Lower is better.
\begin{equation}
    \text{DMR} = \frac{1}{n} \sum_{i \in \mathcal{T}} \mathbb{I}(C_i > d_i)
\end{equation}

\subsubsection{Average GPU Utilization ($\eta$)}
The ratio of the total effective processing time to the total active time of the cluster. Higher is better.
First, we define the \textbf{Makespan} ($C_{\max}$), which is the completion time of the last task in the system:
\begin{equation}
    C_{\max} = \max_{i \in \mathcal{T}} (C_i)
\end{equation}
The utilization is calculated as:
\begin{equation}
    \eta = \frac{\sum_{i \in \mathcal{T}} \sum_{j \in \mathcal{G}} x_{ij} \cdot p_{ij}}{m \cdot (C_{\max} - \min_{k \in \mathcal{T}} a_k)}
\end{equation}
Where $m$ is the total number of GPUs, and the denominator represents the total GPU-time available during the active scheduling window.

\section{Complexity Analysis}

% To justify the necessity of employing heuristic or meta-heuristic algorithms (e.g., Genetic Algorithms, Reinforcement Learning) rather than exact methods for large-scale instances (such as the 20,000 tasks in our dataset), we analyze the computational complexity of the proposed Heterogeneous GPU Scheduling Problem (HGSP).

% \begin{theorem}
The Heterogeneous GPU Scheduling Problem (HGSP) with the objective of minimizing Total Weighted Tardiness is $\mathcal{NP}$-hard.
% \end{theorem}

% \begin{proof_sketch}
We prove the $\mathcal{NP}$-hardness of the HGSP by \textbf{restriction}. We show that a special case of our general problem reduces to the \textit{Single Machine Total Weighted Tardiness Problem} ($1 || \sum w_j T_j$), which is a known $\mathcal{NP}$-hard problem.

Consider a restricted instance of the HGSP with the following constraints:
\begin{enumerate}
    \item \textbf{Single GPU:} The cluster consists of only one GPU ($m=1$).
    \item \textbf{Infinite Memory:} The memory capacity of the GPU is sufficiently large ($M_1 \to \infty$), or task memory demands are zero ($m_i = 0$), effectively removing the memory constraint.
    \item \textbf{Simultaneous Arrival:} All tasks arrive at time zero ($a_i = 0, \forall i \in \mathcal{T}$).
    \item \textbf{Homogeneous Processing:} Since there is only one GPU, the processing time $p_{ij}$ simplifies to a fixed processing time $p_i$ for each task.
\end{enumerate}

Under these restrictions, the decision variables reduce to finding a permutation (sequence) of tasks. The objective function remains:
\begin{equation}
    \text{Minimize } \sum_{i \in \mathcal{T}} w_i \cdot \max(0, C_i - d_i)
\end{equation}
where $C_i$ is determined solely by the sum of processing times of tasks preceding $i$ in the sequence plus $p_i$.

This restricted problem is exactly the \textbf{Single Machine Total Weighted Tardiness Problem}, denoted as $1 || \sum w_j T_j$ in Graham's notation.
It has been proven by Lawler (1977) and Lenstra et al. (1977) that $1 || \sum w_j T_j$ is $\mathcal{NP}$-hard in the strong sense.

Since a special case of the HGSP is $\mathcal{NP}$-hard, the general HGSP (which adds multiple heterogeneous machines, memory constraints, and release times) is at least as hard as this special case. Therefore, the HGSP is $\mathcal{NP}$-hard.
% \end{proof_sketch}

% Lawler, E. L. (1973).
% Optimal sequencing of a single machine subject to precedence constraints.
% Management Science, 19(5), 544–546.

% Lenstra, J. K., Rinnooy Kan, A. H. G., & Brucker, P. (1977).
% Complexity of machine scheduling problems.
% Annals of Discrete Mathematics, 1, 343–362.

\section{Algorithm Design}

Given the online nature of the problem, where tasks arrive dynamically ($a_i$) and full knowledge of future workloads is unavailable, we adopt a dynamic scheduling approach. The scheduler processes tasks based on their arrival order and assigns them to the heterogeneous GPU cluster using different strategies.

We propose three algorithms to schedule tasks to the Heterogeneous GPU cluster: a baseline First-In-First-Out (FIFO) approach, an improved Greedy strategy, and a meta-heuristic Simulated Annealing with Greedy assignment (SAGreedy) algorithm.

\subsection{Baseline Method: FIFO}

The First-In-First-Out (FIFO) scheduler serves as a simple baseline that processes tasks strictly in their arrival order. This approach is straightforward and provides a reference for comparing more sophisticated algorithms.

\subsubsection{Algorithm Logic}
At any scheduling decision point:
\begin{enumerate}
    \item \textbf{Sort}: Sort all tasks in ascending order of their arrival times $a_i$.
    \item \textbf{Assign}: For each task $i$ in order:
    \begin{itemize}
        \item Identify the set of valid GPUs $\mathcal{G}_{valid} = \{j \in \mathcal{G} \mid M_j \ge m_i\}$.
        \item If $\mathcal{G}_{valid} = \emptyset$, skip the task (resource unavailable).
        \item Otherwise, assign task $i$ to the GPU $j^* \in \mathcal{G}_{valid}$ that minimizes the start time.
    \end{itemize}
\end{enumerate}

The start time on GPU $j$ is calculated as $s_{ij} = \max(a_i, \text{avail}_j)$, where $\text{avail}_j$ is the time GPU $j$ finishes its currently assigned tasks. The completion time is $c_{ij} = s_{ij} + \frac{L_i}{v_j}$.

\subsubsection{Complexity Analysis}
Let $N$ be the number of tasks and $M$ be the number of GPUs. The complexity is $O(N \log N + N \cdot M)$ for sorting and GPU assignment.

\subsection{Improved Heuristic: Greedy (EFT)}

The Greedy scheduler improves upon FIFO by considering both arrival time and GPU heterogeneity when making scheduling decisions. It uses an Earliest Finish Time (EFT) strategy that accounts for GPU computing speeds.

\subsubsection{Algorithm Logic}
At any scheduling decision point:
\begin{enumerate}
    \item \textbf{Sort}: Sort all tasks in ascending order of their arrival times $a_i$.
    \item \textbf{Assign}: For each task $i$ in order:
    \begin{itemize}
        \item Identify the set of valid GPUs $\mathcal{G}_{valid} = \{j \in \mathcal{G} \mid M_j \ge m_i\}$.
        \item If $\mathcal{G}_{valid} = \emptyset$, skip the task.
        \item Otherwise, assign task $i$ to the GPU $j^* \in \mathcal{G}_{valid}$ that minimizes the completion time $c_{ij} = \max(a_i, \text{avail}_j) + \frac{L_i}{v_j}$.
    \end{itemize}
\end{enumerate}

\textbf{Key Difference from FIFO}: While FIFO selects the GPU with the earliest start time, Greedy considers the execution time on different GPUs (varying $v_j$) and selects the GPU that minimizes the actual completion time.

\subsubsection{Complexity Analysis}
The complexity remains $O(N \log N + N \cdot M)$, but the EFT strategy better exploits GPU heterogeneity for improved performance.

\subsection{Meta-Heuristic: SAGreedy}

To address the NP-Hardness of minimizing Total Weighted Tardiness (as proven in Section II), we propose a Simulated Annealing with Greedy GPU assignment (SAGreedy) algorithm. This meta-heuristic explores the global solution space of task permutations while using efficient greedy GPU assignment for evaluation.

\subsubsection{Algorithm Overview}
\begin{enumerate}
    \item \textbf{Initialization}: Start with the Greedy solution as the initial state.
    \item \textbf{Temperature Schedule}: Use exponential cooling $T_{k+1} = \alpha \cdot T_k$ where $\alpha \in (0, 1)$.
    \item \textbf{Neighbor Generation}: Generate new task ordering by prioritizing tardy tasks based on current temperature.
    \item \textbf{Acceptance Criterion}: Accept worse solutions with probability $e^{-\Delta/T}$ to escape local optima.
    \item \textbf{GPU Assignment}: For each task ordering, use greedy GPU assignment (EFT) for evaluation.
\end{enumerate}

\subsubsection{Fitness Function}
The fitness function corresponds directly to the minimization objective defined in Eq. (1). For a given task ordering with greedy GPU assignment, the fitness value (to be minimized) is:
\begin{equation}
    Fitness = \sum_{i \in \mathcal{T}} w_i \cdot \max(0, c_i - d_i)
\end{equation}

\subsubsection{Priority-Based Neighbor Generation}
A key innovation in SAGreedy is the temperature-dependent neighbor generation strategy:
\begin{itemize}
    \item \textbf{High Temperature}: Prioritize a larger fraction of tardy tasks, enabling more exploration.
    \item \textbf{Low Temperature}: Prioritize fewer tardy tasks, focusing on exploitation of good solutions.
\end{itemize}

Let $\tau$ be the set of tardy tasks (where $c_i > d_i$). The priority ratio $\rho \in [0.1, 0.8]$ is calculated as:
\begin{equation}
    \rho = \min(0.8, \max(0.1, \frac{T - T_{min}}{T_{initial} - T_{min}}))
\end{equation}

The top $\rho \cdot |\tau|$ tardy tasks are selected and prioritized in the new ordering, while remaining tasks follow arrival-time ordering.

\subsubsection{Complexity Analysis}
Let $N$ be the number of tasks, $M$ be the number of GPUs, $K$ be the maximum iterations, and $T_{initial}$ be the initial temperature.
\begin{itemize}
    \item \textbf{Fitness Evaluation}: For each candidate solution, greedy assignment involves $O(N \cdot M)$ operations.
    \item \textbf{Total Complexity}: $O(K \cdot N \cdot M)$ for the complete annealing process.
\end{itemize}

Although computationally more expensive than FIFO and Greedy ($O(N \log N + N \cdot M)$), SAGreedy allows for escaping local optima by explicitly considering weighted tardiness in the optimization objective.





\section{Experimental Evaluation}

\subsection{Experimental Setup}

\subsubsection{Dataset Description}
We evaluate the proposed algorithms on four datasets with varying characteristics:
\begin{itemize}
    \item \textbf{tasks1}: Course-provided dataset with 1000 tasks, medium load level.
    \item \textbf{tasks2}: Course-provided dataset with 2000 tasks, medium load level.
    \item \textbf{tasks3}: Self-generated dataset with 1000 tasks, high load level (tighter deadlines).
    \item \textbf{tasks4}: Self-generated dataset with 2000 tasks, extreme load level (very tight deadlines).
\end{itemize}

Each task is characterized by workload ($L_i$), memory demand ($m_i$), deadline ($d_i$), weight ($w_i$), and arrival time ($a_i$). The datasets feature heterogeneous task requirements and dynamic arrival patterns.

\subsubsection{Cluster Configuration}
We use three cluster sizes to evaluate scalability:
\begin{itemize}
    \item \textbf{Small}: 3 GPUs (1 A100, 1 A30, 1 L40)
    \item \textbf{Medium}: 6 GPUs (2 of each model)
    \item \textbf{Large}: 9 GPUs (3 of each model)
\end{itemize}

The GPU specifications are based on real NVIDIA hardware: A100 (80GB, 57.0x scaling), A30 (24GB, 30.0x baseline), and L40 (48GB, 55.5x scaling). This heterogeneity forces multi-GPU collaboration as no single GPU can handle all tasks independently.

\subsubsection{Algorithm Parameters}
For the SAGreedy meta-heuristic, we use the following parameters:
\begin{itemize}
    \item Initial temperature: $T_{initial} = 1000$
    \item Cooling rate: $\alpha = 0.95$
    \item Maximum iterations: $K = 100$
    \item Priority ratio range: $\rho \in [0.1, 0.8]$
\end{itemize}

\subsection{Results and Discussion}

Tables \ref{tab:tasks1_results}, \ref{tab:tasks2_results}, and \ref{tab:tasks4_results} present the performance comparison of the three algorithms on representative datasets with small, medium, and large clusters, respectively.

% Table for tasks1 (small cluster)
\begin{table}[htbp]
\caption{Algorithm performance on tasks1 (small cluster)}
\label{tab:tasks1_results}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccccc}
\hline
Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
\hline
FIFO & 22292 & 9.16 & 57 & 5.7\% & 524 & 2503 & 85.6\% & 32.1\% \\
Greedy & 22830 & 9.38 & 52 & 5.2\% & 503 & 2498 & 80.4\% & 27.0\% \\
SAGreedy & 22967 & 9.45 & 49 & 4.9\% & \textbf{393} & 2498 & 80.5\% & 27.3\% \\
\hline
\end{tabular}
}
\end{table}

% Table for tasks2 (medium cluster)
\begin{table}[htbp]
\caption{Algorithm performance on tasks2 (medium cluster)}
\label{tab:tasks2_results}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccccc}
\hline
Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
\hline
FIFO & 16272 & 6.45 & 19 & 1.9\% & 140 & 2493 & 43.1\% & 15.5\% \\
Greedy & 13508 & 5.38 & 1 & 0.1\% & \textbf{0.35} & 2493 & 35.4\% & 7.9\% \\
SAGreedy & 13508 & 5.38 & 1 & 0.1\% & \textbf{0.35} & 2493 & 35.4\% & 7.9\% \\
\hline
\end{tabular}
}
\end{table}

% Table for tasks4 (large cluster, high load)
\begin{table}[htbp]
\caption{Algorithm performance on tasks4 (large cluster)}
\label{tab:tasks4_results}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccccc}
\hline
Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
\hline
FIFO & 2.33M & 382.5 & 1982 & 99.1\% & 2.26M & 1784 & 99.4\% & 38.0\% \\
Greedy & 2.33M & 382.2 & 1982 & 99.1\% & 2.26M & 1777 & 99.7\% & 38.1\% \\
SAGreedy & \textbf{2.30M} & \textbf{385.0} & \textbf{1825} & \textbf{91.3\%} & \textbf{2.23M} & 1803 & 98.3\% & 37.3\% \\
\hline
\end{tabular}
}
\end{table}

\subsubsection{Key Observations}

\textbf{1. Weighted Tardiness Minimization:}
SAGreedy consistently achieves the lowest weighted tardiness (WT) across all datasets. On tasks1, SAGreedy reduces WT by 25\% compared to FIFO and 22\% compared to Greedy. This validates the effectiveness of the simulated annealing approach in optimizing the primary objective.

\textbf{2. Deadline Miss Performance:}
On datasets with moderate load (tasks1, tasks2), all algorithms maintain reasonable deadline miss rates below 6\%. However, under extreme load (tasks4), SAGreedy significantly outperforms the baselines, reducing the deadline miss rate from 99.1\% to 91.3\% by explicitly prioritizing tardy tasks.

\textbf{3. GPU Utilization Trade-offs:}
FIFO achieves higher GPU time utilization (85.6\% vs. 80.4\% for Greedy on tasks1) but at the cost of higher weighted tardiness. This highlights that na{\"i}ve resource occupation does not translate to better scheduling quality when weighted deadlines are considered.

\textbf{4. Scalability:}
As the cluster size increases from 3 to 9 GPUs, the relative performance gap between algorithms narrows on easy instances (tasks2), where all methods achieve near-optimal solutions. However, on hard instances (tasks4), the advantage of SAGreedy becomes more pronounced, demonstrating the value of meta-heuristic search for complex scheduling scenarios.

\subsubsection{Visual Analysis}

Figures \ref{fig:gantt_comparison} and \ref{fig:utilization_comparison} provide visual insights into the scheduling behavior.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_FIFO_first50.png}
\hfill
\includegraphics[width=0.48\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_SAGreedy_first50.png}
\caption{Gantt charts: FIFO vs. SAGreedy (first 50 tasks). Red: deadline misses, green: on-time.}
\label{fig:gantt_comparison}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_comparison.png}
\caption{Algorithm comparison on tasks1 (small cluster).}
\label{fig:utilization_comparison}
\end{figure}

\section{Conclusion}

This paper addresses the Heterogeneous GPU Scheduling Problem (HGSP), an NP-hard optimization problem with practical significance in cloud computing and high-performance computing environments. We formalized the problem with weighted tardiness minimization as the primary objective and proposed three scheduling algorithms:

\textbf{FIFO} provides a simple baseline that processes tasks in arrival order. \textbf{Greedy (EFT)} improves upon FIFO by considering GPU heterogeneity through earliest finish time assignment. \textbf{SAGreedy} combines simulated annealing for task ordering with greedy GPU assignment to explicitly optimize weighted tardiness.

Experimental evaluation on four datasets with varying load levels demonstrates that:
\begin{enumerate}
    \item SAGreedy consistently achieves the lowest weighted tardiness, with up to 25\% improvement over FIFO on moderate-load datasets.
    \item Under extreme load, SAGreedy reduces deadline miss rate by 7.8 percentage points compared to baselines.
    \item GPU utilization alone is an insufficient metric; intelligent scheduling that considers deadlines and priorities significantly improves system performance.
\end{enumerate}

The results validate that meta-heuristic approaches like simulated annealing can effectively navigate the complex solution space of heterogeneous scheduling problems, even with dynamic task arrivals. Future work may explore reinforcement learning for online adaptation and multi-objective optimization considering energy consumption and fairness.

\section{LLM Inference GPU Scheduling}

As an extension to the general heterogeneous GPU scheduling problem, we consider the specialized case of scheduling LLM inference requests on GPU clusters. LLM inference has unique characteristics that distinguish it from general computational workloads, particularly the two-phase execution model (prefill and decode) and the use of tensor parallelism for multi-GPU execution.

\subsection{Problem Formulation}

\subsubsection{LLM Inference Characteristics}

Unlike traditional GPU workloads, LLM inference operates in two distinct phases:

\begin{itemize}
    \item \textbf{Prefill Phase (Compute-Bound)}: Processes the entire input prompt in parallel. This phase is compute-intensive with high arithmetic intensity, processing tokens at $\sim$1000--2000 tokens/s. It generates the key-value (KV) cache required for decoding.

    \item \textbf{Decode Phase (Memory-Bound)}: Generates output tokens auto-regressively using the cached keys and values. This phase is memory bandwidth-limited, processing tokens at $\sim$100--500 tokens/s (single request), but can benefit significantly from continuous batching where multiple decode requests share GPU resources.
\end{itemize}

\subsubsection{Task Model}

The LLM task model differs from the general model in several key aspects:

\textbf{Task Parameters:}
\begin{itemize}
    \item $M_i$: Model name (e.g., ``Qwen3'', ``DeepSeek-R1'', ``Llama3-70B'')
    \item $L_i$: Number of tokens to process (prefill or decode)
    \item $\phi_i \in \{\text{PREFILL}, \text{DECODE}\}$: Execution phase
    \item $m_i$: Total memory requirement (GB) for the model
    \item $\tau_i$: Tensor parallelism degree (number of GPUs required)
    \item $w_i$: Task priority weight
    \item $a_i$: Arrival time
\end{itemize}

\textbf{Derived Parameters:}
\begin{itemize}
    \item $p_{ij}^{\phi} = \frac{L_i}{\lambda_{j}^{\phi}}$: Execution time on GPU $j$ in phase $\phi$, where $\lambda_{j}^{\phi}$ is the token throughput (tokens/s).
    \item For prefill: $\lambda_{j}^{\text{PREFILL}} \approx 1000$ tokens/s
    \item For decode: $\lambda_{j}^{\text{DECODE}} \approx 5000$ tokens/s (simplified model without batching)
\end{itemize}

\textbf{Constraints:}
\begin{itemize}
    \item \textbf{Multi-GPU Allocation}: Each task requires exactly $\tau_i$ GPUs for tensor parallelism
    \item \textbf{Memory Capacity}: $\frac{m_i}{\tau_i} \le M_j$ for each allocated GPU $j$
    \item \textbf{No Deadlines}: LLM inference tasks optimize for response time rather than meeting hard deadlines
\end{itemize}

\subsubsection{Objective Function}

The primary objective is to minimize the \textbf{Total Weighted Waiting Time}:
\begin{equation}
    \text{Minimize } Z = \sum_{i \in \mathcal{T}} w_i \cdot (c_i - a_i)
\end{equation}
where $c_i$ is the completion time and $a_i$ is the arrival time.

Secondary metrics include:
\begin{itemize}
    \item Average Waiting Time: $\frac{1}{n} \sum_{i \in \mathcal{T}} (c_i - a_i)$
    \item Makespan: $C_{\max} = \max_{i \in \mathcal{T}} c_i$
    \item Completion Rate: $\frac{|\{i : c_i \le T_{\max}\}|}{n}$
\end{itemize}

\subsection{Scheduling Algorithms}

We implement and compare four scheduling algorithms adapted for LLM inference:

\subsubsection{FIFO (First-In-First-Out)}
Processes tasks strictly in arrival order. For each task, allocates the first available GPU group with sufficient memory and correct size ($\tau_i$ GPUs).

\subsubsection{WeightedFIFO}
Prioritizes tasks by weight first, then by arrival time. High-priority requests (larger $w_i$) are scheduled before low-priority requests, even if they arrive later.

\subsubsection{SRPT (Shortest Remaining Processing Time)}
Estimates remaining processing time based on token count and phase. Prefers tasks with shorter estimated duration to minimize average completion time.

\subsubsection{WSRPT (Weighted Shortest Remaining Processing Time)}
Combines weight and duration: prioritizes tasks with high weight-to-duration ratio. This optimizes for the weighted waiting time objective.

\subsection{Experimental Evaluation}

\subsubsection{Dataset Description}

Since LLM inference workloads differ significantly from traditional GPU tasks, we design a custom synthetic workload that highlights algorithmic differences:

\textbf{Discriminating Workload Design:}
\begin{itemize}
    \item 20 tasks arrive simultaneously at $t=0$
    \item Cluster: 16 H100 GPUs (can run 8 concurrent tasks with $\tau=2$)
    \item First 8 tasks: Low weight ($w=1$), long duration (2000 tokens)
    \item Next 12 tasks: High weight ($w=10$), short duration (500 tokens)
\end{itemize}

This design forces a scheduling decision: only 8 of 20 tasks can run initially. The choice of which 8 tasks to start determines the weighted waiting time.

\textbf{GPU and Model Specifications:}
\begin{itemize}
    \item GPU: H100 80GB (3350 GB/s bandwidth, 989 TFLOPS compute)
    \item Model: Qwen3 (30B parameters, requires 15GB per GPU with $\tau=2$)
\end{itemize}

\subsubsection{Results}

Table \ref{tab:llm_discriminating_results} presents the performance comparison on the discriminating workload.

\begin{table}[htbp]
\caption{Algorithm performance on discriminating LLM workload (16 H100 GPUs)}
\label{tab:llm_discriminating_results}
\centering
\begin{tabular}{l|ccc}
\hline
Algorithm & Weighted Wait & Avg Wait & Completed \\
\hline
FIFO & 336.400 & 2.402 & 20 \\
WeightedFIFO & 108.040 & 1.802 & 20 \\
SRPT & 108.040 & 1.802 & 20 \\
WSRPT & 108.040 & 1.802 & 20 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Key Observations}

\textbf{1. Priority-Aware Scheduling Dominance:}
WeightedFIFO, SRPT, and WSRPT all achieve identical performance on this workload, reducing weighted waiting time by 67.9\% compared to FIFO. This demonstrates that prioritizing high-weight, short-duration tasks is crucial for LLM inference scheduling.

\textbf{2. FIFO Inefficiency:}
FIFO selects the first 8 tasks by arrival order (all low-weight, long-duration), causing high-weight tasks to wait $\sim$2 seconds. Priority-aware algorithms select high-weight tasks first, completing them quickly before starting low-weight tasks.

\textbf{3. Average Waiting Time:}
SRPT and WSRPT reduce average waiting time by 25.0\% compared to FIFO (1.802s vs. 2.402s), confirming that shorter tasks should be prioritized to improve system responsiveness.

\textbf{4. Algorithm Equivalence:}
On this discriminating workload, WeightedFIFO, SRPT, and WSRPT produce identical schedules because the high-weight tasks are also the short-duration tasks, creating alignment between weight-based and duration-based prioritization.

\subsection{Simplified Discrete-Time Simulator}

To enable clear algorithm comparison, we implement a simplified discrete-time simulator with the following design:

\begin{enumerate}
    \item Time advances in fixed steps ($\Delta t = 0.01$s)
    \item At each step: (1) Complete finished tasks and free GPUs, (2) Add newly arrived tasks to queue, (3) Scheduler selects next task from queue
    \item This ensures scheduling decisions occur when \textbf{multiple tasks are waiting}, making algorithm differences visible
\end{enumerate}

This contrasts with event-driven simulators where tasks are scheduled immediately upon arrival (queue size = 1, no prioritization opportunity). The simplified approach successfully demonstrates the 67.9\% improvement from priority-aware scheduling.

% \appendix
% \section{Additional Experimental Results}

% This appendix provides comprehensive experimental results across all dataset and cluster size combinations. Tables IV-X present detailed performance metrics for configurations not shown in the main text. Figures 3-26 display Gantt chart visualizations for all algorithms.

% \subsection{Additional Performance Metrics}

% Tables IV-X show algorithm performance on remaining dataset/cluster combinations.

% % Table for tasks1 (large cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks1 (large cluster)}
% \label{tab:tasks1_large_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 14938 & 6.07 & 15 & 1.5\% & 83.22 & 2496 & 27.0\% & 8.3\% \\
% Greedy & 13119 & 5.34 & 0 & 0.0\% & 0.00 & 2496 & 23.8\% & 4.9\% \\
% SAGreedy & 13119 & 5.34 & 0 & 0.0\% & 0.00 & 2496 & 23.8\% & 4.9\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks2 (small cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks2 (small cluster)}
% \label{tab:tasks2_small_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 21785 & 8.67 & 52 & 5.2\% & 524.5 & 2495 & 84.5\% & 31.4\% \\
% Greedy & 22342 & 8.92 & 53 & 5.3\% & 504.3 & 2496 & 78.9\% & 25.6\% \\
% SAGreedy & 22418 & 8.97 & 54 & 5.4\% & 454.8 & 2496 & 78.9\% & 25.7\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks3 (small cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks3 (small cluster)}
% \label{tab:tasks3_small_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 4.48M & 995 & 1493 & 99.5\% & 4.40M & 3506 & 99.8\% & 38.5\% \\
% Greedy & 4.47M & 995 & 1494 & 99.6\% & 4.40M & 3501 & 99.9\% & 38.1\% \\
% SAGreedy & 4.02M & 922 & 851 & 56.7\% & 3.96M & 3797 & 92.2\% & 35.3\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks3 (medium cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks3 (medium cluster)}
% \label{tab:tasks3_medium_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 554470 & 123.3 & 1430 & 95.3\% & 480572 & 1757 & 99.6\% & 38.5\% \\
% Greedy & 553083 & 123.0 & 1428 & 95.2\% & 479169 & 1754 & 99.7\% & 38.3\% \\
% SAGreedy & 484357 & 112.0 & 1318 & 87.9\% & 411817 & 1768 & 98.8\% & 37.6\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks3 (large cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks3 (large cluster)}
% \label{tab:tasks3_large_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 35691 & 7.94 & 51 & 3.4\% & 221.2 & 1512 & 78.2\% & 30.5\% \\
% Greedy & 35410 & 7.89 & 33 & 2.2\% & 171.2 & 1503 & 70.0\% & 21.4\% \\
% SAGreedy & 35669 & 7.95 & 27 & 1.8\% & 75.21 & 1503 & 70.1\% & 21.5\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks4 (small cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks4 (small cluster)}
% \label{tab:tasks4_small_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 13.12M & 2149 & 1996 & 99.8\% & 13.04M & 5319 & 100.0\% & 38.1\% \\
% Greedy & 13.12M & 2149 & 1995 & 99.8\% & 13.04M & 5320 & 99.9\% & 37.6\% \\
% SAGreedy & 12.92M & 2135 & 1957 & 97.9\% & 12.84M & 5334 & 99.7\% & 37.8\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% % Table for tasks4 (medium cluster)
% \begin{table}[htbp]
% \caption{Algorithm performance on tasks4 (medium cluster)}
% \label{tab:tasks4_medium_results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccccccc}
% \hline
% Algorithm & TWCT & ACT & DMC & DMR (\%) & WT & Makespan & GPU Util (\%) & Mem Util (\%) \\
% \hline
% FIFO & 5.03M & 823.8 & 1989 & 99.5\% & 4.95M & 2664 & 99.8\% & 38.3\% \\
% Greedy & 5.03M & 823.8 & 1991 & 99.6\% & 4.95M & 2665 & 99.7\% & 38.0\% \\
% SAGreedy & 4.95M & 815.9 & 1884 & 94.2\% & 4.88M & 2683 & 99.0\% & 37.2\% \\
% \hline
% \end{tabular}
% }
% \end{table}

% \subsection{Gantt Charts}

% Figures 3-26 show Gantt chart visualizations for all algorithms across different dataset and cluster configurations. Each row displays FIFO (left), Greedy (center), and SAGreedy (right) results.

% % tasks1 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_SAGreedy_first50.png}
% \caption{tasks1 (small): First 50 tasks.}
% \label{fig:gantt_tasks1_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks1/figures/tasks1_small_gantt_SAGreedy_all.png}
% \caption{tasks1 (small): All tasks.}
% \label{fig:gantt_tasks1_small_all}
% \end{figure}

% % tasks1 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_SAGreedy_first50.png}
% \caption{tasks1 (medium): First 50 tasks.}
% \label{fig:gantt_tasks1_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks1/figures/tasks1_medium_gantt_SAGreedy_all.png}
% \caption{tasks1 (medium): All tasks.}
% \label{fig:gantt_tasks1_medium_all}
% \end{figure}

% % tasks1 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_SAGreedy_first50.png}
% \caption{tasks1 (large): First 50 tasks.}
% \label{fig:gantt_tasks1_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks1/figures/tasks1_large_gantt_SAGreedy_all.png}
% \caption{tasks1 (large): All tasks.}
% \label{fig:gantt_tasks1_large_all}
% \end{figure}

% % tasks2 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_SAGreedy_first50.png}
% \caption{tasks2 (small): First 50 tasks.}
% \label{fig:gantt_tasks2_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks2/figures/tasks2_small_gantt_SAGreedy_all.png}
% \caption{tasks2 (small): All tasks.}
% \label{fig:gantt_tasks2_small_all}
% \end{figure}

% % tasks2 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_SAGreedy_first50.png}
% \caption{tasks2 (medium): First 50 tasks.}
% \label{fig:gantt_tasks2_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks2/figures/tasks2_medium_gantt_SAGreedy_all.png}
% \caption{tasks2 (medium): All tasks.}
% \label{fig:gantt_tasks2_medium_all}
% \end{figure}

% % tasks2 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_SAGreedy_first50.png}
% \caption{tasks2 (large): First 50 tasks.}
% \label{fig:gantt_tasks2_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks2/figures/tasks2_large_gantt_SAGreedy_all.png}
% \caption{tasks2 (large): All tasks.}
% \label{fig:gantt_tasks2_large_all}
% \end{figure}

% % tasks3 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_SAGreedy_first50.png}
% \caption{tasks3 (small): First 50 tasks.}
% \label{fig:gantt_tasks3_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks3/figures/tasks3_small_gantt_SAGreedy_all.png}
% \caption{tasks3 (small): All tasks.}
% \label{fig:gantt_tasks3_small_all}
% \end{figure}

% % tasks3 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_SAGreedy_first50.png}
% \caption{tasks3 (medium): First 50 tasks.}
% \label{fig:gantt_tasks3_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks3/figures/tasks3_medium_gantt_SAGreedy_all.png}
% \caption{tasks3 (medium): All tasks.}
% \label{fig:gantt_tasks3_medium_all}
% \end{figure}

% % tasks3 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_SAGreedy_first50.png}
% \caption{tasks3 (large): First 50 tasks.}
% \label{fig:gantt_tasks3_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks3/figures/tasks3_large_gantt_SAGreedy_all.png}
% \caption{tasks3 (large): All tasks.}
% \label{fig:gantt_tasks3_large_all}
% \end{figure}

% % tasks4 small cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_SAGreedy_first50.png}
% \caption{tasks4 (small): First 50 tasks.}
% \label{fig:gantt_tasks4_small_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_small_tasks4/figures/tasks4_small_gantt_SAGreedy_all.png}
% \caption{tasks4 (small): All tasks.}
% \label{fig:gantt_tasks4_small_all}
% \end{figure}

% % tasks4 medium cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_SAGreedy_first50.png}
% \caption{tasks4 (medium): First 50 tasks.}
% \label{fig:gantt_tasks4_medium_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_medium_tasks4/figures/tasks4_medium_gantt_SAGreedy_all.png}
% \caption{tasks4 (medium): All tasks.}
% \label{fig:gantt_tasks4_medium_all}
% \end{figure}

% % tasks4 large cluster
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_FIFO_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_Greedy_first50.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_SAGreedy_first50.png}
% \caption{tasks4 (large): First 50 tasks.}
% \label{fig:gantt_tasks4_large_first50}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_FIFO_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_Greedy_all.png}
% \includegraphics[width=0.32\textwidth]{../results/sf40_large_tasks4/figures/tasks4_large_gantt_SAGreedy_all.png}
% \caption{tasks4 (large): All tasks.}
% \label{fig:gantt_tasks4_large_all}
% \end{figure}

\end{document}


